{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models\n",
    "\n",
    "A Gaussian Mixture Model (GMM) is a clustering model that fits a dataset using $K$ Gaussian distributions, with means $\\boldsymbol{\\mu}_k$, covariance matrices $\\boldsymbol{\\Sigma}_k$, and weights $\\pi_k$. Thus the model may be written\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf x | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k),\n",
    "$$\n",
    "where the multivariate Gaussian has the form $\\mathcal{N}(\\mathbf x_i | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) = \\bar{N} \\textrm{exp} (-\\frac{1}{2}(\\mathbf x_i - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf x_i - \\boldsymbol{\\mu}_k) )$, where $\\bar{N}$ is the normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood in GMM\n",
    "\n",
    "The log-likelihood is thus\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^{n} \\textrm{log}\\bigg[\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf x | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\bigg].\n",
    "$$\n",
    "\n",
    "Minimizing the loss w.r.t. the means, $\\boldsymbol{\\mu}_k$, yields\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\frac{\\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)} \\Sigma_k^{-1} (x_i - \\mu_k) = 0.\n",
    "$$\n",
    "\n",
    "Let $z_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}$, in which case\n",
    "$$\n",
    "\\sum_{i=1}^{n} z_{ik} \\Sigma_k^{-1} (x_i - \\mu_k) = 0.\n",
    "$$\n",
    "\n",
    "The $\\Sigma_k^{-1}$ term may be removed by left-multiplying both sides with $\\Sigma_k$\n",
    "$$\n",
    "\\sum_{i=1}^{n} z_{ik} (x_i - \\mu_k) = 0.\n",
    "$$\n",
    "\n",
    "Making $\\boldsymbol{\\mu}_k$ the subject of the expression now gives\n",
    "$$\n",
    "\\boldsymbol{\\mu}_k = \\frac{\\sum_{i=1}^{n} z_{ik} \\mathbf{x}_i}{\\sum_{i=1}^{n} z_{ik}}.\n",
    "$$\n",
    "Although this looks like a solution for $\\boldsymbol{\\mu}_k$, this is not quite the case, since the $z_{ik}$ depends on $\\boldsymbol{\\mu}_k$. Therefore this condition is true at the minimium, but it cannot be used to calculate $\\boldsymbol{\\mu}_k$ directly.\n",
    "\n",
    "Following a similar derivation for the weights $\\pi_k$ gives $\\pi_k = \\sum z_{ik} / n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "The Expectation-maximization algorithm works by iteratively alternating between updating $\\mu_k, \\Sigma_k, \\pi_k$ and updating $z_{ik}$:\n",
    "- **E-step**: Compute the posterior probability $z_{ik}$ for each point to be in each Gaussian component.\n",
    "- **M-step**: Update the parameters $(\\mu_k, \\Sigma_k, \\pi_k)$ of each Gaussian using weighted averages.\n",
    "\n",
    "Expectation-maximization is a very generic algorithm to optimize likelihood in probabilistic models with **latent variables**. In GMMs, the latent variables are true class memberships. The E-step computes the posterior over latent variables, conditioned on the parameters of the model. The M-step optimizes the parameters, conditioned on the latent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
