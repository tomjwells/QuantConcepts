{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Concepts\n",
    "\n",
    "## Reward Function\n",
    "The reward function $R$ assigns a numerical reward to each state-action $(S, A)$ pair.\n",
    "\n",
    "$$\n",
    "R: S \\times A  \\to \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "## Action Policy\n",
    "\n",
    "An action policy $Q$ assigns to each state $S$ and allowed action $A$ a numerical value. The numerical value is composed of the *immediate reward* of taking\n",
    "action $A$ and the *discounted delayed reward* - given an optimal action taken in the subsequent state.\n",
    "\n",
    "$$\n",
    "Q: S \\times A  \\to \\mathbb{R},\n",
    "$$\n",
    "$$\n",
    "Q(S_t, A_t) = R(S_t, A_t) + \\gamma\\cdot \\underset{a}{\\text{max}}\\ Q(S_{t+1},a).\n",
    "$$\n",
    "\n",
    "## Representation\n",
    "In general, the optimal action policy $Q$ can not be specified in closed form (e.g. in the form of a table). Therefore, $Q$-learning relies in general on approximate representations for the optimal policy $Q$.\n",
    "\n",
    "## Neural Network\n",
    "Due to the approximation capabilities of neural networks (\"Universal Approximation Theorems\"), neural networks are typically used to represent optimal action policies $Q$. Features are the parameters that describe the state of the environment. Labels are values attached to each allowed action.\n",
    "\n",
    "## Exploration\n",
    "This refers to actions taken by an agent that are random in nature. The purpose is to explore random actions and their associated values beyond what the current optimal policy would dictate.\n",
    "\n",
    "## Exploitation\n",
    "This refers to actions taken in accordance with the current optimal policy.\n",
    "\n",
    "## Replay\n",
    "This refers to the (regular) updating of the optimal action policy given past and memorized experiences (by re-training the neural network).\n",
    "\n",
    "## Important Variables\n",
    "\n",
    "### `gamma`\n",
    "The parameter gamma represents the discount factor by which delayed\n",
    "rewards are taken into account.\n",
    "\n",
    "### `epsilon`\n",
    "The parameter epsilon defines the ratio with which the algorithm relies\n",
    "on exploration as compared to exploitation.\n",
    "\n",
    "### `epsilon_decay`\n",
    "The parameter epsilon_decay specifies the rate at which epsilon is\n",
    "reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt, mpl\n",
    "plt.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpaAI Gym CartPole Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "# env.seed(100)\n",
    "env.action_space.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000e+00 -3.4028e+38 -4.1888e-01 -3.4028e+38], [4.8000e+00 3.4028e+38 4.1888e-01 3.4028e+38], (4,), float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8  ,   -inf, -0.419,   -inf], dtype=float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  ,   inf, 0.419,   inf], dtype=float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.0375,  0.0237,  0.0234, -0.0275], dtype=float32), {})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    # env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "\n",
    "set_seeds(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNAgent:\n",
    "    def __init__(self):\n",
    "        self.max = 0\n",
    "        self.scores = list()\n",
    "        self.memory = list()\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=4, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=keras.optimizers.legacy.RMSprop(learning_rate=0.001))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= 0.5:\n",
    "            return env.action_space.sample()\n",
    "        action = np.where(self.model.predict(state, batch_size=None)[0, 0] > 0.5, 1, 0)\n",
    "        return action\n",
    "\n",
    "    def train_model(self, state, action):\n",
    "        self.model.fit(state, epochs=1, verbose=False)\n",
    "\n",
    "    def learn(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state = env.reset()[0]\n",
    "            for _ in range(201):\n",
    "                state = np.reshape(state, [1, 4])\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, trunc, info = env.step(action)\n",
    "                if done:\n",
    "                    score = _ + 1\n",
    "                    self.scores.append(score)\n",
    "                    self.max = max(score, self.max)\n",
    "                    print('episode: {:4d}/{} | score: {:3d} | max: {:3d}'.format(e, episodes, score, self.max), end='\\r')\n",
    "                    break\n",
    "                self.memory.append((state, action))\n",
    "                self.train_model(state, action)\n",
    "                state = next_state\n",
    "\n",
    "\n",
    "agent = NNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   34/1000 | score:  17 | max:  22\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1000/1000 | score:  16 | max:  45\r"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "agent.learn(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy_score(np\u001b[38;5;241m.\u001b[39mwhere(agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mf\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), l)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy_score(np.where(agent.model.predict(f) > 0.5, 1, 0), l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39maverages))\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpolyval(np\u001b[38;5;241m.\u001b[39mpolyfit(x, agent\u001b[38;5;241m.\u001b[39maverages, deg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m), x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "x = range(len(agent.averages))\n",
    "y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)\n",
    "plt.plot(agent.averages, label='moving average')\n",
    "plt.plot(x, y, 'r--', label='regression')\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('total reward')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class DQLAgent:\n",
    "    def __init__(self, gamma=0.95, hu=24, opt=keras.optimizers.legacy.Adam,\n",
    "                 lr=0.001, finish=False):\n",
    "        self.finish = finish\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = 32\n",
    "        self.max_treward = 0\n",
    "        self.averages = list()\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.osn = env.observation_space.shape[0]\n",
    "        self.model = self._build_model(hu, opt, lr)\n",
    "\n",
    "    def _build_model(self, hu, opt, lr):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hu, input_dim=self.osn,\n",
    "                        activation='relu'))\n",
    "        model.add(Dense(hu, activation='relu'))\n",
    "        model.add(Dense(env.action_space.n, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=opt(learning_rate=lr))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        action = self.model.predict(state)[0]\n",
    "        return np.argmax(action)\n",
    "\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            if not done:\n",
    "                reward += self.gamma * np.amax(\n",
    "                    self.model.predict(next_state)[0])\n",
    "            target = self.model.predict(state)\n",
    "            target[0, action] = reward\n",
    "            self.model.fit(state, target, epochs=1,\n",
    "                           verbose=False)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def learn(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes + 1):\n",
    "            state = env.reset()[0]\n",
    "            state = np.reshape(state, [1, self.osn])\n",
    "            for _ in range(5000):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, trunc, info = env.step(action)\n",
    "                next_state = np.reshape(next_state,\n",
    "                                        [1, self.osn])\n",
    "                self.memory.append([state, action, reward,\n",
    "                                    next_state, done])\n",
    "                state = next_state\n",
    "                if trunc:\n",
    "                    treward = _ + 1\n",
    "                    trewards.append(treward)\n",
    "                    av = sum(trewards[-25:]) / 25\n",
    "                    self.averages.append(av)\n",
    "                    self.max_treward = max(self.max_treward, treward)\n",
    "                    templ = 'episode: {:4d}/{} | treward: {:4d} | '\n",
    "                    templ += 'av: {:6.1f} | max: {:4d}'\n",
    "                    print(templ.format(e, episodes, treward, av,\n",
    "                                       self.max_treward), end='\\r')\n",
    "                    break\n",
    "            if av > 195 and self.finish:\n",
    "                print()\n",
    "                break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "\n",
    "    def test(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes + 1):\n",
    "            state = env.reset()[0]\n",
    "            for _ in range(5001):\n",
    "                state = np.reshape(state, [1, self.osn])\n",
    "                action = np.argmax(self.model.predict(state)[0])\n",
    "                next_state, reward, done, trunc, info = env.step(action)\n",
    "                state = next_state\n",
    "                if trunc:\n",
    "                    treward = _ + 1\n",
    "                    trewards.append(treward)\n",
    "                    print('episode: {:4d}/{} | treward: {:4d}'\n",
    "                          .format(e, episodes, treward), end='\\r')\n",
    "                    break\n",
    "        return trewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
