{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(100)\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from keras. layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "tf.random.set_random_seed(100)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "env = gym.make ( 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 22:20:38.413665: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "  def __init__(self, finish=False):\n",
    "    self.finish = finish\n",
    "    self.epsilon = 1.0\n",
    "    self.epsilon_min = 0.01\n",
    "    self.epsilon_decay = 0.995\n",
    "    self.gamma = 0.95\n",
    "    self.batch_size = 32\n",
    "    self.max_treward = 0\n",
    "    self.averages = list()\n",
    "    self.memory = deque(maxlen=2000)\n",
    "    self.osn = env.observation_space.shape[0]\n",
    "    self.model = self.create_model()\n",
    "\n",
    "  def create_model(self):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=self.osn, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(env.action_space.n, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "      return env.action_space.sample()\n",
    "    action = self.model.predict(state)\n",
    "    return np.argmax(action)\n",
    "  \n",
    "  def replay(self):\n",
    "    batch = random.sample(self.memory, self.batch_size)\n",
    "    for state, action, reward, next_state, done in batch:\n",
    "      if not done:\n",
    "        reward += self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "      target = self.model.predict(state)\n",
    "      target[0, action] = reward\n",
    "      self.model.fit(state, target, epochs=1, verbose=False)\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "      self.epsilon *= self.epsilon_decay\n",
    "\n",
    "  def learn(self, episodes=1000):\n",
    "    trewards = []\n",
    "    for e in range(episodes):\n",
    "      state = env.reset()\n",
    "      state = np.reshape(state, [1, self.osn])\n",
    "      for time in range(5000):\n",
    "        action = self.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, self.osn])\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        if done:\n",
    "          treward = time + 1\n",
    "          trewards.append(treward)\n",
    "          av = sum(trewards[:25]) / 25 # Work out a Rolling average\n",
    "          self.averages.append(av)\n",
    "          self.max_treward = max(self.max_treward, treward)\n",
    "          print('Episode: {}/{}, treward: {}, av: {:.2}, max: {:2}'.format(e, episodes, treward, av, self.max_treward))          \n",
    "          break\n",
    "      if av > 199 and self.finish:\n",
    "        print('Completed after {} episodes'.format(e))\n",
    "        break\n",
    "      if len(self.memory) > self.batch_size:\n",
    "        self.replay()\n",
    "\n",
    "  def test(self, episodes=100):\n",
    "    trewards = []\n",
    "    for e in range(episodes):\n",
    "      state = env.reset()\n",
    "      state = np.reshape(state, [1, self.osn])\n",
    "      for time in range(5000):\n",
    "        env.render()\n",
    "        action = np.argmax(self.model.predict(state)[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "          treward = time + 1\n",
    "          trewards.append(treward)\n",
    "          print('Episode: {}/{}, treward: {}'.format(e, episodes, treward))\n",
    "          break\n",
    "    env.close()\n",
    "\n",
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
