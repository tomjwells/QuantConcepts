{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method\n",
    "\n",
    "The Hessian is the matrix of second-order partial derivatives of a function,\n",
    "$$\n",
    "\t(\\mathbf{H})_{ij} = H_{ij} = \\frac{\\partial^2 f(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j} \n",
    "$$\n",
    "where $\\mathbf{\\theta} = (\\theta_1,\\theta_2,\\dots)$ is the vector of parameters on which the function depends.\n",
    "\n",
    "The most basic second-order optimization algorithm is Newton's algorithm, which consists of updates of the form\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\mathbf{H}_k^{-1} \\mathbf{g}_k\n",
    "$$\n",
    "\n",
    "This algorithm is derived by making a second-order Taylor series approximation of $f(\\theta)$ around $\\theta_k$:\n",
    "$$\n",
    "f_{\\text{quad}}(\\theta) = f(\\theta_k) + \\mathbf{g}_k^T (\\theta - \\theta_k) + \\frac{1}{2} (\\theta - \\theta_k)^T \\mathbf{H}_k (\\theta - \\theta_k)\n",
    "$$\n",
    "\n",
    "Differentiating and equating to zero to solve for $\\theta_{k+1}$:\n",
    "$$\n",
    "\\nabla f_{\\text{quad}}(\\theta) = 0 + \\mathbf{g}_k + \\mathbf{H}_k (\\theta - \\theta_k) = 0\n",
    "$$\n",
    "$$\n",
    "-\\mathbf{g}_k = \\mathbf{H}_k (\\theta - \\theta_k)\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta_k - \\mathbf{H}_k^{-1} \\mathbf{g}_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
