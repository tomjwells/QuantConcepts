{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood & Defining a Loss Function\n",
    "\n",
    "Consider a Bernoulli random variable. That is, a process that yields event $Y = 1$ with probability $p$ and event $Y = 0$ with probability $1 - p$.\n",
    "\n",
    "The likelihood is given by\n",
    "$$\n",
    "\\prod_{i | y_i = 1} h(\\mathbf{x}_i) \\cdot \\prod_{i | y_i = 0} \\left(1 - h(\\mathbf{x}_i)\\right),\n",
    "$$\n",
    "where $y_i$ are the outcomes of the trials in our training set, and $h(\\mathbf{x}_i)$ is the prediction generated by the model given feature set $\\mathbf{x}_i$.\n",
    "\n",
    "Thus the negative log-likelihood can be written\n",
    "\\begin{align}\n",
    "\\mathcal{L} =& - \\sum_{i | y_i = 1} \\log h(\\mathbf{x}_i) - \\sum_{i | y_i = 0} \\log \\left(1 - h(\\mathbf{x}_i)\\right) \\notag\\\\\n",
    "            =&- \\sum_i \\left[ y_i \\log h(\\mathbf{x}_i) + (1 - y_i) \\log \\left(1 - h(\\mathbf{x}_i)\\right) \\right].\n",
    "\\end{align}\n",
    "\n",
    "In summary, the loss function is:\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_i \\left[ y_i \\log h(\\mathbf{x}_i) + (1 - y_i) \\log \\left(1 - h(\\mathbf{x}_i)\\right) \\right],\n",
    "$$\n",
    "where\n",
    "$$\n",
    "h(\\mathbf{x}) = \\frac{1}{1 + e^{-\\boldsymbol{\\beta}^\\top \\mathbf{x}}}.\n",
    "$$\n",
    "\n",
    "This loss function is convex, meaning optimization algorithms such as gradient descent have guaranteed convergence properties. The bad news is that unlike multiple linear regression, the minimum is not available in closed form.\n",
    "\n",
    "Usually, logistic regression is optimized using 2nd order methods (Newtonâ€™s method), but for simplicity, we will consider gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Logistic Regression\n",
    "\n",
    "Let $g(x)$ denote the sigmoid function. It is simple to show that for the sigmoid function,\n",
    "$$\n",
    "g'(x) = g(x)(1 - g(x)).\n",
    "$$\n",
    "\n",
    "In the following lines we precompute useful terms for the gradient of the loss fuction.\n",
    "\\begin{align}\n",
    "\\nabla \\log h(\\mathbf{x}) &= \\nabla \\log g(\\boldsymbol{\\beta}^\\top \\mathbf{x}) = (1 - h(\\mathbf{x})) \\mathbf{x}, \\\\\n",
    "\\nabla \\log (1 - h(\\mathbf{x})) &= \\nabla \\log (1 - g(\\boldsymbol{\\beta}^\\top \\mathbf{x})) = - h(\\mathbf{x}) \\mathbf{x}.\n",
    "\\end{align}\n",
    "\n",
    "Now, taking the gradient of the loss function,\n",
    "\\begin{align}\n",
    "\\nabla \\mathcal{L} &= - \\nabla \\sum_i \\left[ y_i \\log h(\\mathbf{x}_i) + (1 - y_i) \\log \\left(1 - h(\\mathbf{x}_i)\\right) \\right] \\notag \\\\\n",
    "                   &= - \\sum_i \\left( y_i - h(\\mathbf{x}_i) \\right) \\mathbf{x}_i \\notag \\\\\n",
    "                   &= - \\mathbf{X}^\\top (\\mathbf{y} - \\hat{\\mathbf{y}}),\n",
    "\\end{align}\n",
    "where $\\mathbf{y}$ is a vector of the resposes, and $\\hat{\\mathbf{y}}$ is a vector of predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression\n",
    "\n",
    "Logistic regression can be generalized to the multinomial case using a **softmax** function.\n",
    "\n",
    "Each class $ i = \\{0, 1, \\dots, K-1\\} $ gets assigned its own $ \\boldsymbol{\\beta}_i $, where $K$ is the number of classes. Then the probability of class $ y = k $ is given by:\n",
    "$$\n",
    "P(y = k) = \\frac{e^{\\boldsymbol{\\beta}_k^\\top \\mathbf{x}}}{\\sum_{i} e^{\\boldsymbol{\\beta}_i^\\top \\mathbf{x}}}.\n",
    "$$\n",
    "\n",
    "Overspecification: Adding any vector $ \\boldsymbol{\\phi} $ to all $ \\boldsymbol{\\beta}_i $ vectors will not change the probabilities. This degree of freedom allows us to constrain one of the $\\boldsymbol{\\beta}_k$, for example, we may set $\\boldsymbol{\\beta}_{K-1} = 0 $.\n",
    "\n",
    "For the binary case, this softmax formulation becomes equivalent to the logistic regression described previously in the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
