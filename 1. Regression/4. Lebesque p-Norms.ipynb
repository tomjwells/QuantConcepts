{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lebesque $p$-Norms\n",
    "\n",
    "The $L$ in $L_p$ norms stands for \"Lebesgue,\" named after Henri Lebesgue, a French mathematician known for his work on integration and measure theory. $L_p$ norms are a family of norms in a vector space, where \"$p$\" is a parameter that determines the type of norm. The most common ones are:\n",
    "- **L1 norm**: Also known as the \"taxicab\" or \"Manhattan\" norm, it sums the absolute values of the components.\n",
    "- **L2 norm**: Also known as the Euclidean norm, it calculates the square root of the sum of the squares of the components.\n",
    "\n",
    "The $L_p$ norm generalizes these, defined as the $p$-th root of the sum of the absolute values of the components raised to the power of $p$. So, you can think of as a formula where you raise each component to the power of $p$, sum them up, and then take the $p$-th root of the total. When $p$ equals 1 or 2, you get the $L_1$ and $L_2$ norms, respectively.\n",
    "\n",
    "Lebesque $p$-Norms exist for any positive integer $n$. However, as $n$ approaches infinity, the norm converges to what's called the L-infinity norm, which is the maximum absolute value among the components.\n",
    "\n",
    "# Effect of Lasso and Ridge Regression on Parameters\n",
    "\n",
    "Ridge regression and Lasso regression are types of linear regression that include regularization to prevent overfitting, especially when dealing with multicollinearity or when the number of predictors exceeds the number of observations. Here's how they work:\n",
    "\n",
    "## **Ridge Regression**\n",
    "- **Also known as**: Tikhonov regularization or L2 regularization.\n",
    "- **Penalty term**: Adds the squared magnitude of the coefficients (multiplied by a regularization parameter lambda) to the loss function.\n",
    "- **Effect on coefficients**: Shrinks the coefficients towards zero but never exactly to zero. This means all variables remain in the model, but their impact is reduced.\n",
    "- **When to use**: Useful when you have many small/medium-sized effects that you don't want to exclude.\n",
    "\n",
    "## **Lasso Regression**\n",
    "- **Also known as**: Least Absolute Shrinkage and Selection Operator or L1 regularization.\n",
    "- **Penalty term**: Adds the absolute value of the coefficients (multiplied by lambda) to the loss function.\n",
    "- **Effect on coefficients**: Can shrink some coefficients to exactly zero, effectively performing **variable selection**.\n",
    "- **When to use**: Helpful when you want a simpler, more interpretable model, as it tends to select a subset of the provided predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# How to calculate/estimate a value for Lambda\n",
    "\n",
    "Choosing lambda in ridge and lasso regression is crucial for balancing the trade-off between bias and variance. There are a few approaches to finding an appropriate lambda:\n",
    "\n",
    "  * Cross-Validation\n",
    "    - **K-fold Cross-Validation**: The dataset is divided into $k$ subsets. The model is trained on $k-1$ of these and validated on the remaining subset. This process is repeated $k$ times, and the average performance is used to select lambda.\n",
    "    - **Leave-One-Out Cross-Validation**: A special case of $k$-fold where $k$ equals the number of data points. It's more computationally expensive but can be effective for smaller datasets.\n",
    "  * Information Criteria\n",
    "    - **Akaike Information Criterion (AIC)** and **Bayesian Information Criterion (BIC)**: These criteria balance model fit with model complexity. Lower values indicate a better model.\n",
    "  * Algorithms\n",
    "    - **Grid Search**: You define a range of lambda values, and the algorithm evaluates the model's performance at each point. The lambda that provides the best performance according to a chosen metric is selected.\n",
    "    - **Random Search**: Similar to grid search but samples lambda values randomly from a specified range. It's often more efficient for high-dimensional spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
